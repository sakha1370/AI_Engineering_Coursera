{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Building Advanced Transformers**\n",
    "\n",
    "**Estimated time needed:  30 minutes**  \n",
    "\n",
    "In this lab, you will implement and experiment with advanced Transformer models using Keras. \n",
    "\n",
    "**Learning objectives:** \n",
    "\n",
    "By the end of this lab, you will: \n",
    "\n",
    "- Implement advanced Transformer models using Keras. \n",
    "\n",
    "- Apply Transformers to real-world sequential data tasks. \n",
    "\n",
    "- Build, train, and evaluate Transformer models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions: \n",
    "\n",
    "### Step 1: Import necessary libraries \n",
    "\n",
    "Before you start, you need to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.12/site-packages (2.16.2)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-19.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.25.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Downloading pyarrow-19.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m123.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-19.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m124.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 tzdata-2025.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m145.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow pyarrow \n",
    "%pip install pandas  \n",
    "%pip install scikit-learn \n",
    "%pip install matplotlib \n",
    "%pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 03:27:56.339232: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-18 03:27:56.340289: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-18 03:27:56.344315: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-18 03:27:56.355838: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-18 03:27:56.382077: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-18 03:27:56.382131: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-18 03:27:56.397007: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-18 03:27:57.407664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setup the Environment to generate synthetic stock price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic stock_prices.csv created and loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a synthetic stock price dataset\n",
    "np.random.seed(42)\n",
    "data_length = 2000  # Adjust data length as needed\n",
    "trend = np.linspace(100, 200, data_length)\n",
    "noise = np.random.normal(0, 2, data_length)\n",
    "synthetic_data = trend + noise\n",
    "\n",
    "# Create a DataFrame and save as 'stock_prices.csv'\n",
    "data = pd.DataFrame(synthetic_data, columns=['Close'])\n",
    "data.to_csv('stock_prices.csv', index=False)\n",
    "print(\"Synthetic stock_prices.csv created and loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1899, 100, 1)\n",
      "Shape of Y: (1899,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "data = pd.read_csv('stock_prices.csv') \n",
    "data = data[['Close']].values \n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for training\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 100\n",
    "X, Y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`tensorflow` is the main library for machine learning in Python.  \n",
    "\n",
    "`stock_prices.csv` is the data set that is loaded. \n",
    "\n",
    "`MinMaxScaler` method is used to normalize the data.  \n",
    "\n",
    "`create_dataset`method is used to prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Multi-Head Self-Attention \n",
    "\n",
    "Define the Multi-Head Self-Attention mechanism. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    "\n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The MultiHeadSelfAttention layer implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. \n",
    "\n",
    "- The attention parameter computes the attention scores and weighted sum of the values. \n",
    "\n",
    "- The split_heads parameter splits the input into multiple heads for parallel attention computation. \n",
    "\n",
    "- The call method applies the self-attention mechanism and combines the heads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Transformer block \n",
    "\n",
    "Define the Transformer block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "\n",
    "- The TransformerBlock layer combines multi-head self-attention with a feed-forward neural network and normalization layers.  \n",
    "\n",
    "- Dropout is used to prevent overfitting. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network with residual connections and layer normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Encoder Layer \n",
    "\n",
    "Define the Encoder layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(EncoderLayer, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The EncoderLayer is similar to the TransformerBlock but is a reusable layer in the Transformer architecture. \n",
    "\n",
    "- It consists of a MultiHeadSelfAttention mechanism followed by a feedforward neural network. \n",
    "\n",
    "- Both sub-layers have residual connections around them, and layer normalization is applied to the output of each sub-layer. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network, with residual connections and layer normalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement Transformer encoder \n",
    "\n",
    "Define the Transformer Encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout \n",
    "\n",
    "class MultiHeadSelfAttention(Layer): \n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    " \n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    "class TransformerBlock(Layer): \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n",
    "class TransformerEncoder(Layer): \n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.embed_dim = embed_dim \n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n",
    "        self.dropout = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs \n",
    "        for i in range(self.num_layers): \n",
    "            x = self.enc_layers[i](x, training=training) \n",
    "        return x \n",
    "\n",
    "# Example usage \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "inputs = tf.random.uniform((1, 100, embed_dim)) \n",
    "outputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training' \n",
    "print(outputs.shape)  # Should print (1, 100, 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The TransformerEncoder is composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build and Compile the Transformer model \n",
    "\n",
    "Integrate the Transformer Encoder into a complete model for sequential data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the necessary parameters \n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The Transformer Encoder model defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output.  \n",
    "\n",
    "- The model is then compiled with the Adam optimizer and mean squared error loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the Transformer model \n",
    "\n",
    "Train the model on the prepared dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - loss: 15.6750 \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - loss: 0.2274 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - loss: 0.1892 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 3s/step - loss: 0.1313\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 3s/step - loss: 0.1872\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.1262 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.1885 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.1240 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.2002 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.1466 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0951 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0975 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0740 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0582 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0676 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0597 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0470 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0378\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0332   \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0256 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fdae0136720>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The model is trained on the normalized stock price data for 20 epochs with a batch size of 32. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate and Make Predictions \n",
    "\n",
    "Evaluate the model's performance and make predictions on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 316ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions \n",
    "predictions = model.predict(X) \n",
    "predictions = scaler.inverse_transform(predictions) \n",
    " \n",
    "\n",
    "# Plot the predictions \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(data, label='True Data') \n",
    "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions') \n",
    "plt.xlabel('Time') \n",
    "plt.ylabel('Stock Price') \n",
    "plt.legend() \n",
    "plt.show() \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The model's predictions are transformed back to the original scale using the inverse transform of the scaler. \n",
    "\n",
    "- The true data and predictions are plotted to visualize the model's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises: \n",
    "\n",
    " ### Exercise 1: Add dropout to the Transformer model \n",
    "\n",
    " **Objective: Understand how to add dropout layers to the Transformer model to prevent overfitting.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Add a dropout layer after the Flatten layer in the model. \n",
    "\n",
    "- Set the dropout rate to 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 1s/step - loss: 5.0307  \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.9480 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.4935 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.2185 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0869 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0682 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0556 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0442 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0402 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0347 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0344 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0269 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0320 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0231 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0235 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0249 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0230 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0253 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0200 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0192\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 304ms/step - loss: 0.0319\n",
      "Test loss: 0.03908465430140495\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "dropout = Dropout(0.5)(flatten) \n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "# Build the model \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Train the model \n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "# Evaluate the model \n",
    "loss = model.evaluate(X, Y) \n",
    "print(f'Test loss: {loss}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "  \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') \n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with different batch sizes \n",
    "\n",
    "**Objective: Observe the impact of different batch sizes on model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Train the model with a batch size of 16. \n",
    "\n",
    "- Train the model with a batch size of 64. \n",
    "\n",
    "- Compare the training time and performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 612ms/step - loss: 0.0400 \n",
      "Epoch 2/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 621ms/step - loss: 0.0284 \n",
      "Epoch 3/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 620ms/step - loss: 0.0295 \n",
      "Epoch 4/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 621ms/step - loss: 0.0366 \n",
      "Epoch 5/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 614ms/step - loss: 0.0223 \n",
      "Epoch 6/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 617ms/step - loss: 0.0242 \n",
      "Epoch 7/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 621ms/step - loss: 0.0342 \n",
      "Epoch 8/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 632ms/step - loss: 0.0185 \n",
      "Epoch 9/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 624ms/step - loss: 0.0161 \n",
      "Epoch 10/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 614ms/step - loss: 0.0171 \n",
      "Epoch 11/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 620ms/step - loss: 0.0111 \n",
      "Epoch 12/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 619ms/step - loss: 0.0091 \n",
      "Epoch 13/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 615ms/step - loss: 0.0079 \n",
      "Epoch 14/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 608ms/step - loss: 0.0113 \n",
      "Epoch 17/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 616ms/step - loss: 0.0183 \n",
      "Epoch 18/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 617ms/step - loss: 0.0099 \n",
      "Epoch 19/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 618ms/step - loss: 0.0107 \n",
      "Epoch 20/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 616ms/step - loss: 0.0300 \n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 314ms/step - loss: 0.0027\n",
      "Test loss with batch size 16: 0.0013662747805938125\n",
      "Epoch 1/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - loss: 0.0109 \n",
      "Epoch 2/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - loss: 0.0038\n",
      "Epoch 3/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0031\n",
      "Epoch 4/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0027\n",
      "Epoch 5/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0026\n",
      "Epoch 6/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0025\n",
      "Epoch 7/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0022 \n",
      "Epoch 8/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0025\n",
      "Epoch 9/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0021\n",
      "Epoch 10/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - loss: 0.0024\n",
      "Epoch 11/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0020\n",
      "Epoch 12/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0024\n",
      "Epoch 13/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0018\n",
      "Epoch 14/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0021\n",
      "Epoch 15/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0023\n",
      "Epoch 16/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0019\n",
      "Epoch 17/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - loss: 0.0019   \n",
      "Epoch 18/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0020\n",
      "Epoch 19/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0020\n",
      "Epoch 20/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0019\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 311ms/step - loss: 6.9128e-04\n",
      "Test loss with batch size 64: 0.0014699293533340096\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different activation function \n",
    "\n",
    " **Objective: Understand how different activation functions impact the model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Change the activation function of the Dense layer to `tanh`. \n",
    "\n",
    "- Train and evaluate the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 1s/step - loss: 0.4703 \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.2989 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.3064 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.2992 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.2873 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.2999 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.2967 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.2913 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.2847 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.2892 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.2890 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.2885   \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.2935 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.2940 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.2999 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.2962 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.2996 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.3057\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.3003 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.2870 \n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 316ms/step - loss: 0.5205\n",
      "Test loss with tanh activation: 0.29668867588043213\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this lab! In this lab, you have built an advanced Transformer model using Keras and applied it to a time series forecasting task. You have learned how to define and implement multi-head self-attention, Transformer blocks, encoder layers, and integrate them into a complete Transformer model. By experimenting with different configurations and training the model, you can further improve its performance and apply it to various sequential data tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "28ac4fd81c1d713f83dcd1cdf1d3383ad25ea92873288fe9e978e9a17b314709"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
